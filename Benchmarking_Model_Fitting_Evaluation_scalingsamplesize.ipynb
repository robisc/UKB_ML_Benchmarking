{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 0. Installing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/dnanexus/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping scikit-learn as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping scikit-survival as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: The directory '/home/dnanexus/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m268.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn) (1.9.3)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m322.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.9/site-packages (from scikit-learn) (1.23.5)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 threadpoolctl-3.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: The directory '/home/dnanexus/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting scikit-survival\n",
      "  Downloading scikit_survival-0.23.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m204.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn<1.6,>=1.4.0\n",
      "  Downloading scikit_learn-1.5.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m250.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting ecos\n",
      "  Downloading ecos-2.0.14-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (218 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.7/218.7 kB\u001b[0m \u001b[31m298.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting osqp!=0.6.0,!=0.6.1\n",
      "  Downloading osqp-0.6.7.post3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.4/297.4 kB\u001b[0m \u001b[31m316.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from scikit-survival) (1.4.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from scikit-survival) (1.23.5)\n",
      "Collecting numexpr\n",
      "  Downloading numexpr-2.10.2-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (396 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.7/396.7 kB\u001b[0m \u001b[31m319.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from scikit-survival) (1.5.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.9/site-packages (from scikit-survival) (1.9.3)\n",
      "Collecting qdldl\n",
      "  Downloading qdldl-0.1.7.post5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m274.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas>=1.4.0->scikit-survival) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas>=1.4.0->scikit-survival) (2024.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn<1.6,>=1.4.0->scikit-survival) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas>=1.4.0->scikit-survival) (1.16.0)\n",
      "Installing collected packages: numexpr, scikit-learn, qdldl, ecos, osqp, scikit-survival\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.6.1\n",
      "    Uninstalling scikit-learn-1.6.1:\n",
      "      Successfully uninstalled scikit-learn-1.6.1\n",
      "Successfully installed ecos-2.0.14 numexpr-2.10.2 osqp-0.6.7.post3 qdldl-0.1.7.post5 scikit-learn-1.5.2 scikit-survival-0.23.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: The directory '/home/dnanexus/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting lifelines\n",
      "  Downloading lifelines-0.30.0-py3-none-any.whl (349 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.3/349.3 kB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting autograd>=1.5\n",
      "  Downloading autograd-1.7.0-py3-none-any.whl (52 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m190.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from lifelines) (1.23.5)\n",
      "Requirement already satisfied: matplotlib>=3.0 in /opt/conda/lib/python3.9/site-packages (from lifelines) (3.6.3)\n",
      "Collecting pandas>=2.1\n",
      "  Downloading pandas-2.2.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m257.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting autograd-gamma>=0.3\n",
      "  Downloading autograd-gamma-0.5.0.tar.gz (4.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting formulaic>=0.2.2\n",
      "  Downloading formulaic-1.1.1-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.7/115.7 kB\u001b[0m \u001b[31m271.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.7.0 in /opt/conda/lib/python3.9/site-packages (from lifelines) (1.9.3)\n",
      "Collecting interface-meta>=1.2.0\n",
      "  Downloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.9/site-packages (from formulaic>=0.2.2->lifelines) (4.12.2)\n",
      "Collecting wrapt>=1.0\n",
      "  Downloading wrapt-1.17.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.6/82.6 kB\u001b[0m \u001b[31m268.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=3.0->lifelines) (1.3.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=3.0->lifelines) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=3.0->lifelines) (3.1.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=3.0->lifelines) (23.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=3.0->lifelines) (0.12.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=3.0->lifelines) (10.4.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=3.0->lifelines) (4.53.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=3.0->lifelines) (2.9.0)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.8/346.8 kB\u001b[0m \u001b[31m313.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas>=2.1->lifelines) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines) (1.16.0)\n",
      "Building wheels for collected packages: autograd-gamma\n",
      "  Building wheel for autograd-gamma (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4032 sha256=aa9e6a517cef21ab61627532d7318ef5c1d975cae6556527e69d3cc8328efe97\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-_p4ury6z/wheels/a8/03/64/8557323821d25118c3a2dc1646996f7a962a8970d4b7d22473\n",
      "Successfully built autograd-gamma\n",
      "Installing collected packages: wrapt, tzdata, interface-meta, autograd, pandas, autograd-gamma, formulaic, lifelines\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.5.3\n",
      "    Uninstalling pandas-1.5.3:\n",
      "      Successfully uninstalled pandas-1.5.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dxdata 0.41.0 requires pandas==1.5.3; python_version >= \"3.8\", but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed autograd-1.7.0 autograd-gamma-0.5.0 formulaic-1.1.1 interface-meta-1.3.0 lifelines-0.30.0 pandas-2.2.3 tzdata-2025.1 wrapt-1.17.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: The directory '/home/dnanexus/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (1.4.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: The directory '/home/dnanexus/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting xgboost\n",
      "  Downloading xgboost-2.1.3-py3-none-manylinux_2_28_x86_64.whl (153.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.9/153.9 MB\u001b[0m \u001b[31m255.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from xgboost) (1.9.3)\n",
      "Collecting nvidia-nccl-cu12\n",
      "  Downloading nvidia_nccl_cu12-2.25.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 MB\u001b[0m \u001b[31m328.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from xgboost) (1.23.5)\n",
      "Installing collected packages: nvidia-nccl-cu12, xgboost\n",
      "Successfully installed nvidia-nccl-cu12-2.25.1 xgboost-2.1.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: The directory '/home/dnanexus/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torchsurv\n",
      "  Downloading torchsurv-0.1.4-py3-none-any.whl (52 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m133.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torch\n",
      "  Downloading torch-2.6.0-cp39-cp39-manylinux1_x86_64.whl (766.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m328.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from torchsurv) (1.9.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from torchsurv) (1.23.5)\n",
      "Collecting torchmetrics\n",
      "  Downloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m338.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.2.0\n",
      "  Downloading triton-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m327.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m317.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.4.5.8\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m320.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m319.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.4.127\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m259.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m320.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.9/site-packages (from torch->torchsurv) (4.12.2)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m323.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /opt/conda/lib/python3.9/site-packages (from torch->torchsurv) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from torch->torchsurv) (3.1.4)\n",
      "Collecting nvidia-nccl-cu12==2.21.5\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m322.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.4.127\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m301.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading fsspec-2025.2.0-py3-none-any.whl (184 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.5/184.5 kB\u001b[0m \u001b[31m281.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparselt-cu12==0.6.2\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m323.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m324.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.4.127\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m342.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.4.127\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m271.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sympy==1.13.1\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m278.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.4.127\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m321.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m318.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting lightning-utilities>=0.8.0\n",
      "  Downloading lightning_utilities-0.12.0-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: packaging>17.1 in /opt/conda/lib/python3.9/site-packages (from torchmetrics->torchsurv) (23.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from lightning-utilities>=0.8.0->torchmetrics->torchsurv) (65.6.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from jinja2->torch->torchsurv) (2.1.1)\n",
      "Installing collected packages: triton, nvidia-cusparselt-cu12, mpmath, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchmetrics, torchsurv\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.25.1\n",
      "    Uninstalling nvidia-nccl-cu12-2.25.1:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.25.1\n",
      "Successfully installed filelock-3.17.0 fsspec-2025.2.0 lightning-utilities-0.12.0 mpmath-1.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.6.0 torchmetrics-1.6.1 torchsurv-0.1.4 triton-3.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: The directory '/home/dnanexus/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting lightgbm\n",
      "  Downloading lightgbm-4.5.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m185.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.9/site-packages (from lightgbm) (1.23.5)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from lightgbm) (1.9.3)\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall scikit-learn scikit-survival -y\n",
    "\n",
    "!pip install scikit-learn\n",
    "!pip install scikit-survival\n",
    "\n",
    "!pip install lifelines\n",
    "\n",
    "!pip install joblib\n",
    "\n",
    "!pip install xgboost\n",
    "\n",
    "!pip install torchsurv\n",
    "\n",
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sksurv\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sksurv.linear_model import CoxnetSurvivalAnalysis\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis\n",
    "from sksurv.util import Surv\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.exceptions import FitFailedWarning\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "from torchsurv.loss.cox import neg_partial_log_likelihood\n",
    "from torchsurv.metrics.cindex import ConcordanceIndex\n",
    "\n",
    "import itertools\n",
    "from itertools import product\n",
    "\n",
    "import lifelines\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "\n",
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1 Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1.1 COXPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_cox(train_data, train_labels, test_data, test_labels, verbose=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Trains a standard CoxPH model without regularization (minimal alpha for numerical stability).\n",
    "    - Re-trains model on whole training split.\n",
    "    - Evaluates performance on the test split via Harrel's C.\n",
    "    - Returns final refit model, timing information, and c-index for evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    labels_array = np.array([(status, time) for status, time in zip(train_labels.iloc[:, 0], train_labels.iloc[:, 1])],\n",
    "                            dtype=[('event', '?'), ('time', '<f8')])\n",
    "\n",
    "    # Step 1: Model fit\n",
    "    if verbose:\n",
    "        print(\"\\nFitting the standard Cox Proportional Hazards model...\")\n",
    "\n",
    "    start_fit_time = time.time()\n",
    "    cox_model = CoxPHSurvivalAnalysis(alpha=1e-6)\n",
    "    cox_model.fit(train_data, labels_array)\n",
    "    end_fit_time = time.time()\n",
    "    fit_time = end_fit_time - start_fit_time\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Fitting took {fit_time:.2f} seconds.\")\n",
    "\n",
    "    timing_info = {\n",
    "        'fit_time': fit_time\n",
    "    }\n",
    "\n",
    "    # Step 2: Harrel's C on the test split\n",
    "    if verbose:\n",
    "        print(\"\\nEvaluating model performance on the test split...\")\n",
    "\n",
    "    test_labels_array = np.array([(status, time) for status, time in zip(test_labels.iloc[:, 0], test_labels.iloc[:, 1])],\n",
    "                                 dtype=[('event', '?'), ('time', '<f8')])\n",
    "\n",
    "    test_predictions = cox_model.predict(test_data)\n",
    "\n",
    "    c_index = concordance_index_censored(test_labels_array[\"event\"], test_labels_array[\"time\"], test_predictions)[0]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Concordance index (c-index) on test split: {c_index:.4f}\")\n",
    "\n",
    "    return cox_model, timing_info, c_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1.2 EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_opt_EN(train_data, train_labels, test_data, test_labels,\n",
    "                 l1_ratios=np.linspace(0.1, 1.0, 10), alphas=np.logspace(-3, 0, 5), max_iter=100, cv_folds=5, verbose=True):\n",
    "    \"\"\"\n",
    "    EN model hyperparameter optimization:\n",
    "        - Uses a fixed grid of alphas for tuning.\n",
    "        - 5-fold CV along an alpha-lambda grid.\n",
    "        - Determines optimal alpha and l1_ratio.\n",
    "        - Re-trains model on the whole training split using optimal settings.\n",
    "        - Evaluates performance on the test split.\n",
    "        - Returns final refit model, CV results, timing information, and c-index for evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    warnings.simplefilter(\"ignore\", UserWarning)\n",
    "    warnings.simplefilter(\"ignore\", FitFailedWarning)\n",
    "    \n",
    "    labels_array = np.array([(status, time) for status, time in zip(train_labels.iloc[:, 0], train_labels.iloc[:, 1])],\n",
    "                            dtype=[('event', '?'), ('time', '<f8')])\n",
    "\n",
    "    # Step 1: Set up CV grid\n",
    "    if verbose:\n",
    "        print(f\"Using fixed alpha grid ranging from {alphas.min():.5f} to {alphas.max():.5f} with 5 alphas.\")\n",
    "\n",
    "    param_grid = {\n",
    "        'l1_ratio': l1_ratios,\n",
    "        'alphas': [[alpha] for alpha in alphas]\n",
    "    }\n",
    "\n",
    "    cv = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Step 2: Hyperparameter optimisation\n",
    "    if verbose:\n",
    "        print(\"Starting hyperparameter optimization using GridSearchCV...\")\n",
    "\n",
    "    start_cv_time = time.time()\n",
    "    grid_search = GridSearchCV(\n",
    "        CoxnetSurvivalAnalysis(max_iter=max_iter),\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=1 if verbose else 0\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(train_data, labels_array)\n",
    "    end_cv_time = time.time()\n",
    "    cv_time = end_cv_time - start_cv_time\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Hyperparameter optimization took {cv_time:.2f} seconds.\")\n",
    "\n",
    "    best_model = grid_search.best_estimator_ \n",
    "    best_l1_ratio = grid_search.best_params_['l1_ratio']\n",
    "    best_alpha = grid_search.best_params_['alphas'][0]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nBest l1_ratio: {best_l1_ratio:.2f}, Best alpha: {best_alpha:.5f}\")\n",
    "\n",
    "    # Step 3: Model fit\n",
    "    if verbose:\n",
    "        print(\"\\nRefitting the model on the entire training set with optimal parameters...\")\n",
    "\n",
    "    start_refit_time = time.time()\n",
    "    final_model = CoxnetSurvivalAnalysis(l1_ratio=best_l1_ratio, alphas=[best_alpha], max_iter=max_iter)\n",
    "    final_model.fit(train_data, labels_array)\n",
    "    end_refit_time = time.time()\n",
    "    refit_time = end_refit_time - start_refit_time\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Refitting took {refit_time:.2f} seconds.\")\n",
    "\n",
    "    cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "    timing_info = {\n",
    "        'cv_time': cv_time,\n",
    "        'refit_time': refit_time\n",
    "    }\n",
    "\n",
    "    # Step 4: Harrel's C on the test split\n",
    "    if verbose:\n",
    "        print(\"\\nEvaluating model performance on the test split...\")\n",
    "\n",
    "    test_labels_array = np.array([(status, time) for status, time in zip(test_labels.iloc[:, 0], test_labels.iloc[:, 1])],\n",
    "                                 dtype=[('event', '?'), ('time', '<f8')])\n",
    "\n",
    "    test_predictions = final_model.predict(test_data)\n",
    "    \n",
    "    c_index = concordance_index_censored(test_labels_array[\"event\"], test_labels_array[\"time\"], test_predictions)[0]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Concordance index (c-index) on test split: {c_index:.4f}\")\n",
    "\n",
    "    return final_model, cv_results, timing_info, c_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1.3 Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_opt_ridge(train_data, train_labels, test_data, test_labels,\n",
    "                    alphas=np.logspace(-3, 0, 5), cv_folds=5, verbose=True):\n",
    "    \"\"\"\n",
    "    Ridge (Cox Proportional Hazards) model hyperparameter optimization:\n",
    "        - Uses a fixed grid of alphas for tuning.\n",
    "        - 5-fold CV along an alpha grid.\n",
    "        - Determines optimal alpha.\n",
    "        - Re-trains model on the whole training split using optimal settings.\n",
    "        - Evaluates performance on the test split.\n",
    "        - Returns final refit model, CV results, timing information, and c-index for evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    warnings.simplefilter(\"ignore\", UserWarning)\n",
    "    warnings.simplefilter(\"ignore\", FitFailedWarning)\n",
    "    \n",
    "    labels_array = np.array([(status, time) for status, time in zip(train_labels.iloc[:, 0], train_labels.iloc[:, 1])],\n",
    "                            dtype=[('event', '?'), ('time', '<f8')])\n",
    "\n",
    "\n",
    "    # Step 1: Set up CV grid\n",
    "    if verbose:\n",
    "        print(f\"Using fixed alpha grid ranging from {alphas.min():.5f} to {alphas.max():.5f} with {len(alphas)} alphas.\")\n",
    "\n",
    "    param_grid = {\n",
    "        'alpha': alphas\n",
    "    }\n",
    "\n",
    "    cv = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Step 2: Hyperparameter optimisation\n",
    "    if verbose:\n",
    "        print(\"Starting hyperparameter optimization using GridSearchCV...\")\n",
    "\n",
    "    start_cv_time = time.time()\n",
    "    grid_search = GridSearchCV(\n",
    "        CoxPHSurvivalAnalysis(),\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=1 if verbose else 0\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(train_data, labels_array)\n",
    "    end_cv_time = time.time()\n",
    "    cv_time = end_cv_time - start_cv_time\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Hyperparameter optimization took {cv_time:.2f} seconds.\")\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_alpha = grid_search.best_params_['alpha']\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nBest alpha: {best_alpha:.5f}\")\n",
    "\n",
    "    # Step 3: Model fit\n",
    "    if verbose:\n",
    "        print(\"\\nRefitting the model on the entire training set with optimal alpha...\")\n",
    "\n",
    "    start_refit_time = time.time()\n",
    "    final_model = CoxPHSurvivalAnalysis(alpha=best_alpha)\n",
    "    final_model.fit(train_data, labels_array)\n",
    "    end_refit_time = time.time()\n",
    "    refit_time = end_refit_time - start_refit_time\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Refitting took {refit_time:.2f} seconds.\")\n",
    "\n",
    "    cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "    timing_info = {\n",
    "        'cv_time': cv_time,\n",
    "        'refit_time': refit_time\n",
    "    }\n",
    "\n",
    "    # Step 4: Harrel's C on the test split\n",
    "    if verbose:\n",
    "        print(\"\\nEvaluating model performance on the test split...\")\n",
    "\n",
    "    test_labels_array = np.array([(status, time) for status, time in zip(test_labels.iloc[:, 0], test_labels.iloc[:, 1])],\n",
    "                                 dtype=[('event', '?'), ('time', '<f8')])\n",
    "\n",
    "    test_predictions = final_model.predict(test_data)\n",
    "\n",
    "    c_index = concordance_index_censored(test_labels_array[\"event\"], test_labels_array[\"time\"], test_predictions)[0]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Concordance index (c-index) on test split: {c_index:.4f}\")\n",
    "\n",
    "    return final_model, cv_results, timing_info, c_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1.4 LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_opt_lasso(train_data, train_labels, test_data, test_labels,\n",
    "                    alphas=np.logspace(-3, 0, 5), max_iter=100, cv_folds=5, verbose=True):\n",
    "    \"\"\"\n",
    "    Lasso model hyperparameter optimization:\n",
    "        - Uses a fixed grid of alphas for tuning.\n",
    "        - 5-fold CV along an alpha grid.\n",
    "        - Determines optimal alpha.\n",
    "        - Re-trains model on the whole training split using optimal settings.\n",
    "        - Evaluates performance on the test split.\n",
    "        - Returns final refit model, CV results, timing information, and c-index for evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    warnings.simplefilter(\"ignore\", UserWarning)\n",
    "    warnings.simplefilter(\"ignore\", FitFailedWarning)\n",
    "\n",
    "    labels_array = np.array([(status, time) for status, time in zip(train_labels.iloc[:, 0], train_labels.iloc[:, 1])],\n",
    "                            dtype=[('event', '?'), ('time', '<f8')])\n",
    "\n",
    "    # Step 1: Set up CV grid\n",
    "    if verbose:\n",
    "        print(f\"Using fixed alpha grid ranging from {alphas.min():.5f} to {alphas.max():.5f} with {len(alphas)} alphas.\")\n",
    "\n",
    "    param_grid = {\n",
    "        'alphas': [[alpha] for alpha in alphas]\n",
    "    }\n",
    "\n",
    "    cv = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Step 2: Hyperparameter optimisation\n",
    "    if verbose:\n",
    "        print(\"Starting hyperparameter optimization using GridSearchCV...\")\n",
    "\n",
    "    start_cv_time = time.time()\n",
    "    grid_search = GridSearchCV(\n",
    "        CoxnetSurvivalAnalysis(l1_ratio=1.0, max_iter=max_iter),  # Set l1_ratio to 1.0 for Lasso\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=1 if verbose else 0\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(train_data, labels_array)\n",
    "    end_cv_time = time.time()\n",
    "    cv_time = end_cv_time - start_cv_time\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Hyperparameter optimization took {cv_time:.2f} seconds.\")\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_alpha = grid_search.best_params_['alphas'][0]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nBest alpha: {best_alpha:.5f}\")\n",
    "\n",
    "    # Step 3: Model fit\n",
    "    if verbose:\n",
    "        print(\"\\nRefitting the model on the entire training set with optimal alpha...\")\n",
    "\n",
    "    start_refit_time = time.time()\n",
    "    final_model = CoxnetSurvivalAnalysis(l1_ratio=1.0, alphas=[best_alpha], max_iter=max_iter)\n",
    "    final_model.fit(train_data, labels_array)\n",
    "    end_refit_time = time.time()\n",
    "    refit_time = end_refit_time - start_refit_time\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Refitting took {refit_time:.2f} seconds.\")\n",
    "\n",
    "    cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "    timing_info = {\n",
    "        'cv_time': cv_time,\n",
    "        'refit_time': refit_time\n",
    "    }\n",
    "\n",
    "    # Step 4: Harrel's C on the test split\n",
    "    if verbose:\n",
    "        print(\"\\nEvaluating model performance on the test split...\")\n",
    "\n",
    "    test_labels_array = np.array([(status, time) for status, time in zip(test_labels.iloc[:, 0], test_labels.iloc[:, 1])],\n",
    "                                 dtype=[('event', '?'), ('time', '<f8')])\n",
    "\n",
    "    test_predictions = final_model.predict(test_data)\n",
    "\n",
    "    c_index = concordance_index_censored(test_labels_array[\"event\"], test_labels_array[\"time\"], test_predictions)[0]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Concordance index (c-index) on test split: {c_index:.4f}\")\n",
    "\n",
    "    return final_model, cv_results, timing_info, c_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1.5 Random Survival Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_opt_RF(train_data, train_labels, test_data, test_labels,\n",
    "                 n_estimators_range=[50, 100, 200], max_depth_range=[3, 7, 10], \n",
    "                 cv_folds=5, verbose=True):\n",
    "    \"\"\"\n",
    "    Random Survival Forest (RSF) model hyperparameter optimization:\n",
    "        - Uses a fixed grid of hyperparameters for tuning.\n",
    "        - 5-fold CV along hyperparameter grid.\n",
    "        - Determines optimal settings for n_estimators, max_depth, etc.\n",
    "        - Re-trains model on the whole training split using optimal settings.\n",
    "        - Evaluates performance on the test split.\n",
    "        - Returns final refit model, CV results, timing information, and c-index for evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    warnings.simplefilter(\"ignore\", UserWarning)\n",
    "    warnings.simplefilter(\"ignore\", FitFailedWarning)\n",
    "\n",
    "    labels_array = np.array([(status, time) for status, time in zip(train_labels.iloc[:, 0], train_labels.iloc[:, 1])],\n",
    "                            dtype=[('event', '?'), ('time', '<f8')])\n",
    "\n",
    "    # Step 1: Set up CV grid\n",
    "    param_grid = {\n",
    "        'n_estimators': n_estimators_range,\n",
    "        'max_depth': max_depth_range\n",
    "    }\n",
    "\n",
    "    cv = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Step 2: Hyperparameter optimisation\n",
    "    if verbose:\n",
    "        print(\"Starting hyperparameter optimization using GridSearchCV...\")\n",
    "\n",
    "    start_cv_time = time.time()\n",
    "    grid_search = GridSearchCV(\n",
    "        RandomSurvivalForest(min_samples_leaf = 10),\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=10 if verbose else 0,\n",
    "        pre_dispatch = '2*n_jobs'\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(train_data, labels_array)\n",
    "    end_cv_time = time.time()\n",
    "    cv_time = end_cv_time - start_cv_time\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Hyperparameter optimization took {cv_time:.2f} seconds.\")\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_n_estimators = grid_search.best_params_['n_estimators']\n",
    "    best_max_depth = grid_search.best_params_['max_depth']\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nBest n_estimators: {best_n_estimators}, Best max_depth: {best_max_depth}\")\n",
    "\n",
    "    # Step 3: Model fit\n",
    "    if verbose:\n",
    "        print(\"\\nRefitting the model on the entire training set with optimal parameters...\")\n",
    "\n",
    "    start_refit_time = time.time()\n",
    "    final_model = RandomSurvivalForest(n_estimators=best_n_estimators, max_depth=best_max_depth, min_samples_leaf=10)\n",
    "    final_model.fit(train_data, labels_array)\n",
    "    end_refit_time = time.time()\n",
    "    refit_time = end_refit_time - start_refit_time\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Refitting took {refit_time:.2f} seconds.\")\n",
    "\n",
    "    cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "    timing_info = {\n",
    "        'cv_time': cv_time,\n",
    "        'refit_time': refit_time\n",
    "    }\n",
    "\n",
    "    # Step 4: Harrel's C on the test split\n",
    "    if verbose:\n",
    "        print(\"\\nEvaluating model performance on the test split...\")\n",
    "\n",
    "    test_labels_array = np.array([(status, time) for status, time in zip(test_labels.iloc[:, 0], test_labels.iloc[:, 1])],\n",
    "                                 dtype=[('event', '?'), ('time', '<f8')])\n",
    "\n",
    "    test_predictions = final_model.predict(test_data)\n",
    "\n",
    "    c_index = concordance_index_censored(test_labels_array[\"event\"], test_labels_array[\"time\"], test_predictions)[0]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Concordance index (c-index) on test split: {c_index:.4f}\")\n",
    "\n",
    "    return final_model, cv_results, timing_info, c_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.5.2 Ranger implementation via skranger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting skranger\n",
      "  Downloading skranger-0.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=1.0 in /opt/conda/lib/python3.9/site-packages (from skranger) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=1.0->skranger) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=1.0->skranger) (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=1.0->skranger) (1.23.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=1.0->skranger) (1.4.2)\n",
      "Installing collected packages: skranger\n",
      "Successfully installed skranger-0.8.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install skranger\n",
    "\n",
    "from skranger.ensemble import RangerForestSurvival\n",
    "from sksurv.util import Surv\n",
    "from sklearn.model_selection import KFold\n",
    "from sksurv.metrics import concordance_index_censored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_opt_RangerRF(train_data, train_labels, test_data, test_labels,\n",
    "                     num_trees_range=[50, 100, 200],\n",
    "                     max_depth_range=[3, 7, 10],\n",
    "                     cv_folds=5, verbose=True):\n",
    "    \"\"\"\n",
    "    Random Survival Forest (RSF) model hyperparameter optimization using skranger:\n",
    "    - Performs grid search over num_trees and max_depth\n",
    "    - Uses K-Fold cross-validation to determine best params\n",
    "    - Re-trains the best model on the full training data\n",
    "    - Evaluates on test data and returns results\n",
    "    \"\"\"\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "    T_train = train_labels.iloc[:, 1].values  # Time-to-event\n",
    "    E_train = train_labels.iloc[:, 0].values  # Event indicator\n",
    "    y_train_structured = Surv.from_arrays(event=E_train, time=T_train)\n",
    "\n",
    "    # Step 1: Set up CV grid\n",
    "    param_grid = [(num_trees, max_depth) for num_trees in num_trees_range for max_depth in max_depth_range]\n",
    "    cv = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    best_c_index = -np.inf\n",
    "    best_params = param_grid[0]\n",
    "    all_cv_results = []\n",
    "    \n",
    "    # Step 2: Hyperparameter optimisation\n",
    "    for num_trees, max_depth in param_grid:\n",
    "        if verbose:\n",
    "            print(f\"\\nTesting num_trees={num_trees}, max_depth={max_depth}...\")\n",
    "        \n",
    "        c_indices = []\n",
    "        start_cv_time = time.time()\n",
    "\n",
    "        for train_idx, val_idx in cv.split(train_data):\n",
    "            train_X, val_X = train_data.iloc[train_idx], train_data.iloc[val_idx]\n",
    "            train_y, val_y = y_train_structured[train_idx], y_train_structured[val_idx]\n",
    "\n",
    "            rsf = RangerForestSurvival(n_estimators=num_trees, max_depth=max_depth, min_node_size=20, seed = 42, oob_error = False)\n",
    "            rsf.fit(train_X, train_y)\n",
    "            \n",
    "            predictions = rsf.predict(val_X)\n",
    "            c_index = concordance_index_censored(val_y['event'], val_y['time'], predictions)[0]\n",
    "            c_indices.append(c_index)\n",
    "\n",
    "        mean_c_index = np.mean(c_indices)\n",
    "        end_cv_time = time.time()\n",
    "\n",
    "        all_cv_results.append({\n",
    "            'num_trees': num_trees,\n",
    "            'max_depth': max_depth,\n",
    "            'mean_c_index': mean_c_index,\n",
    "            'cv_time': end_cv_time - start_cv_time\n",
    "        })\n",
    "\n",
    "        if mean_c_index > best_c_index:\n",
    "            best_c_index = mean_c_index\n",
    "            best_params = (num_trees, max_depth)\n",
    "\n",
    "    cv_results = pd.DataFrame(all_cv_results)\n",
    "\n",
    "    # Step 3: Model fit\n",
    "    best_num_trees, best_max_depth = best_params\n",
    "    if verbose:\n",
    "        print(f\"\\nBest num_trees: {best_num_trees}, Best max_depth: {best_max_depth}\")\n",
    "\n",
    "    start_refit_time = time.time()\n",
    "    final_model = RangerForestSurvival(n_estimators=best_num_trees, max_depth=best_max_depth, min_node_size=20, seed = 42, oob_error = False)\n",
    "    final_model.fit(train_data, y_train_structured)\n",
    "    end_refit_time = time.time()\n",
    "\n",
    "    refit_time = end_refit_time - start_refit_time\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Refitting took {refit_time:.2f} seconds.\")\n",
    "\n",
    "    # Step 4: Harrel's C on the test split\n",
    "    T_test = test_labels.iloc[:, 1].values\n",
    "    E_test = test_labels.iloc[:, 0].values\n",
    "    y_test_structured = Surv.from_arrays(event=E_test, time=T_test)\n",
    "\n",
    "    test_predictions = final_model.predict(test_data)\n",
    "    c_index = concordance_index_censored(y_test_structured['event'], y_test_structured['time'], test_predictions)[0]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Concordance index (c-index) on test split: {c_index:.4f}\")\n",
    "\n",
    "    timing_info = {'refit_time': refit_time}\n",
    "\n",
    "    return final_model, cv_results, timing_info, c_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1.6 GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.6.1 LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def cox_ph_loss_lgb(preds, dataset):\n",
    "    labels = dataset.get_label()\n",
    "    survival_time = np.abs(labels)\n",
    "    event = (labels > 0).astype(int)\n",
    "\n",
    "    #sort by survival time\n",
    "    sorted_indices = np.argsort(survival_time)\n",
    "    preds = preds[sorted_indices]\n",
    "    event = event[sorted_indices]\n",
    "    survival_time = survival_time[sorted_indices]\n",
    "\n",
    "    #compute risk set using Breslow's method for ties\n",
    "    exp_preds = np.exp(preds)\n",
    "    risk_set_sum = np.zeros_like(exp_preds)\n",
    "    accumulated_sum = 0.0\n",
    "    last_time = None\n",
    "    last_exp_p = 0.0\n",
    "\n",
    "    for i in reversed(range(len(preds))):\n",
    "        if last_time is None or survival_time[i] < last_time:\n",
    "            accumulated_sum += last_exp_p \n",
    "            last_exp_p = 0.0  \n",
    "        last_exp_p += exp_preds[i] \n",
    "        risk_set_sum[i] = accumulated_sum + last_exp_p \n",
    "        last_time = survival_time[i]\n",
    "\n",
    "    grad = np.zeros_like(preds)\n",
    "    hess = np.zeros_like(preds)\n",
    "\n",
    "    r_k = 0.0\n",
    "    s_k = 0.0\n",
    "    for i in range(len(preds)):\n",
    "        if event[i] == 1:\n",
    "            r_k += 1.0 / max(risk_set_sum[i], 1e-10)\n",
    "            s_k += 1.0 / max(risk_set_sum[i] ** 2, 1e-10)\n",
    "\n",
    "        exp_p = exp_preds[i]\n",
    "        grad[i] = exp_p * r_k - event[i]\n",
    "        hess[i] = exp_p * (r_k - exp_p * s_k)\n",
    "\n",
    "    #map back to original order\n",
    "    grad_unsorted = np.zeros_like(grad)\n",
    "    hess_unsorted = np.zeros_like(hess)\n",
    "    grad_unsorted[sorted_indices] = grad\n",
    "    hess_unsorted[sorted_indices] = hess\n",
    "\n",
    "    return grad_unsorted, hess_unsorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def cindex_metric(preds, dataset):\n",
    "    \"\"\"\n",
    "    Harrel's C evaluation metric for LightGBM\n",
    "    \n",
    "    args:\n",
    "        preds (np.ndarray): Predicted risk scores \n",
    "        dataset (lightgbm.Dataset): LightGBM dataset containing labels.\n",
    "\n",
    "    returns:\n",
    "        tuple: (metric name, metric value, is_higher_better)\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    labels = dataset.get_label()\n",
    "    \n",
    "    survival_time = np.abs(labels) \n",
    "    event = (labels > 0).astype(int)  \n",
    "    \n",
    "    cindex = concordance_index_censored(event.astype(bool), survival_time, preds)[0]\n",
    "    \n",
    "    return 'cindex', cindex, True \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_opt_lightGBM(\n",
    "    train_data, train_labels, test_data, test_labels,\n",
    "    n_estimators_range=[50, 100, 200],\n",
    "    num_leaves_range=[7, 127, 1023],\n",
    "    cv_folds=5, verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Optimized LightGBM Cox model hyperparameter tuning:\n",
    "        - Tests a hyperparameter grid in a CV approach.\n",
    "        - Records results for all tested combinations and steps.\n",
    "    \"\"\"\n",
    "\n",
    "    #encode survival times for compativility with LightGBM: Positive for events, Negative for censoring\n",
    "    y_train_encoded = np.array([\n",
    "        survival_time if event == 1 else -survival_time\n",
    "        for survival_time, event in zip(train_labels[f\"{endpoint}_followup\"], train_labels[f\"{endpoint}_status\"])\n",
    "    ])\n",
    "    y_test_encoded = np.array([\n",
    "        survival_time if event == 1 else -survival_time\n",
    "        for survival_time, event in zip(test_labels[f\"{endpoint}_followup\"], test_labels[f\"{endpoint}_status\"])\n",
    "    ])\n",
    "\n",
    "    dtrain = lgb.Dataset(\n",
    "        train_data,\n",
    "        label=y_train_encoded,\n",
    "        free_raw_data=False\n",
    "    )\n",
    "\n",
    "    # Step 1: Set up CV grid\n",
    "    param_grid = product(\n",
    "        n_estimators_range,\n",
    "        num_leaves_range,\n",
    "    )\n",
    "\n",
    "    best_params = None\n",
    "    best_c_index = -np.inf\n",
    "    best_std = None\n",
    "    best_model = None\n",
    "    cv_times = []\n",
    "\n",
    "    all_cv_results = []\n",
    "    \n",
    "\n",
    "    # Step 2: Hyperparameter optimisation\n",
    "    for n_estimators, num_leaves in param_grid:\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nTesting n_estimators={n_estimators}, num_leaves={num_leaves}...\")\n",
    "            \n",
    "        params = {\n",
    "            'objective': cox_ph_loss_lgb,\n",
    "            'boosting_type': 'gbdt',\n",
    "            'learning_rate': 0.1,\n",
    "            'num_leaves': num_leaves,\n",
    "            'min_data_in_leaf': 10,\n",
    "            'max_depth': -1,  # allow unrestricted depth as tuned by num_leaves\n",
    "            'metric': 'None',\n",
    "            'seed': 42,\n",
    "            'verbose': -1\n",
    "        }\n",
    "\n",
    "        start_cv_time = time.time()\n",
    "        \n",
    "        cv_results = lgb.cv(\n",
    "            params=params,\n",
    "            train_set=dtrain,\n",
    "            num_boost_round=n_estimators,\n",
    "            nfold=cv_folds,\n",
    "            stratified=False,\n",
    "            shuffle=True,\n",
    "            metrics=None,\n",
    "            feval=cindex_metric,\n",
    "            seed=42\n",
    "        )\n",
    "        cv_time = time.time() - start_cv_time\n",
    "        cv_times.append(cv_time)\n",
    "\n",
    "        mean_cindex = cv_results['valid cindex-mean'][-1]\n",
    "        std_cindex = cv_results['valid cindex-stdv'][-1]\n",
    "\n",
    "        all_cv_results.append({\n",
    "            'n_estimators': n_estimators,\n",
    "            'num_leaves': num_leaves,\n",
    "            'mean_cindex': mean_cindex,\n",
    "            'std_cindex': std_cindex,\n",
    "            'cv_time': cv_time\n",
    "        })\n",
    "\n",
    "        if mean_cindex > best_c_index:\n",
    "            best_c_index = mean_cindex\n",
    "            best_std = std_cindex\n",
    "            best_params = params\n",
    "            best_params['n_estimators'] = n_estimators  # Add n_estimators to best_params\n",
    "\n",
    "    cv_results_df = pd.DataFrame(all_cv_results)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nBest parameters: {best_params}\")\n",
    "        print(f\"Mean CV C-index: {best_c_index:.4f} ± {best_std:.4f}\")\n",
    "\n",
    "    # Step 3: Model fit\n",
    "    if verbose:\n",
    "        print(\"\\nRefitting the model on the entire training set with optimal parameters...\")\n",
    "    start_refit_time = time.time()\n",
    "    final_model = lgb.train(\n",
    "        params=best_params,\n",
    "        train_set=dtrain,\n",
    "        num_boost_round=best_params['n_estimators']\n",
    "    )\n",
    "    refit_time = time.time() - start_refit_time\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Refitting took {refit_time:.2f} seconds.\")\n",
    "\n",
    "    timing_info = {\n",
    "        'cv_time': sum(cv_times),\n",
    "        'refit_time': refit_time\n",
    "    }\n",
    "\n",
    "    # Step 4: Harrel's C on the test split\n",
    "    test_predictions = final_model.predict(test_data)\n",
    "    y_event_test = (y_test_encoded > 0).astype(int)\n",
    "    y_time_test = np.abs(y_test_encoded)\n",
    "    c_index = concordance_index_censored(\n",
    "        y_event_test.astype(bool),\n",
    "        y_time_test,\n",
    "        test_predictions\n",
    "    )[0]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Concordance index (c-index) on test split: {c_index:.4f}\")\n",
    "\n",
    "    return final_model, cv_results_df, timing_info, c_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.1.6.2 sksurv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_opt_GBM(train_data, train_labels, test_data, test_labels,\n",
    "                  n_estimators_range=[50, 100, 200],\n",
    "                  max_depth_range=[3, 7, 10],\n",
    "                  cv_folds=5, verbose=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Gradient Boosting Machine (GBM) model hyperparameter optimization for survival analysis:\n",
    "        - Uses a fixed grid of hyperparameters for tuning.\n",
    "        - 5-fold CV along hyperparameter grid.\n",
    "        - Determines optimal settings for learning_rate, n_estimators, max_depth, etc.\n",
    "        - Re-trains model on the whole training split using optimal settings.\n",
    "        - Evaluates performance on the test split.\n",
    "        - Returns final refit model, CV results, timing information, and c-index for evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    warnings.simplefilter(\"ignore\", UserWarning)\n",
    "    warnings.simplefilter(\"ignore\", FitFailedWarning)\n",
    "\n",
    "    labels_array = np.array([(status, time) for status, time in zip(train_labels.iloc[:, 0], train_labels.iloc[:, 1])],\n",
    "                            dtype=[('event', '?'), ('time', '<f8')])\n",
    "\n",
    "    \n",
    "    # Step 1: Set up CV grid\n",
    "    param_grid = {\n",
    "        'n_estimators': n_estimators_range,\n",
    "        'max_depth': max_depth_range,\n",
    "    }\n",
    "\n",
    "    cv = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Step 2: Hyperparameter optimisation\n",
    "    if verbose:\n",
    "        print(\"Starting hyperparameter optimization using GridSearchCV...\")\n",
    "\n",
    "    start_cv_time = time.time()\n",
    "    grid_search = GridSearchCV(\n",
    "        GradientBoostingSurvivalAnalysis(learning_rate = 0.1, min_samples_leaf = 10),\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=10 if verbose else 0,\n",
    "        pre_dispatch='2*n_jobs'\n",
    "    )\n",
    "\n",
    "    grid_search.fit(train_data, labels_array)\n",
    "    end_cv_time = time.time()\n",
    "    cv_time = end_cv_time - start_cv_time\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Hyperparameter optimization took {cv_time:.2f} seconds.\")\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_n_estimators = grid_search.best_params_['n_estimators']\n",
    "    best_max_depth = grid_search.best_params_['max_depth']\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nBest n_estimators: {best_n_estimators}, \"\n",
    "              f\"Best max_depth: {best_max_depth}\")\n",
    "\n",
    "    # Step 3: Model fit\n",
    "    if verbose:\n",
    "        print(\"\\nRefitting the model on the entire training set with optimal parameters...\")\n",
    "\n",
    "    start_refit_time = time.time()\n",
    "    final_model = GradientBoostingSurvivalAnalysis(\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=best_n_estimators,\n",
    "        max_depth=best_max_depth,\n",
    "        min_samples_leaf=10\n",
    "    )\n",
    "    final_model.fit(train_data, labels_array)\n",
    "    end_refit_time = time.time()\n",
    "    refit_time = end_refit_time - start_refit_time\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Refitting took {refit_time:.2f} seconds.\")\n",
    "\n",
    "    cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "    timing_info = {\n",
    "        'cv_time': cv_time,\n",
    "        'refit_time': refit_time\n",
    "    }\n",
    "\n",
    "    # Step 4: Harrel's C on the test split\n",
    "    if verbose:\n",
    "        print(\"\\nEvaluating model performance on the test split...\")\n",
    "\n",
    "    test_labels_array = np.array([(status, time) for status, time in zip(test_labels.iloc[:, 0], test_labels.iloc[:, 1])],\n",
    "                                 dtype=[('event', '?'), ('time', '<f8')])\n",
    "\n",
    "    test_predictions = final_model.predict(test_data)\n",
    "\n",
    "    c_index = concordance_index_censored(test_labels_array[\"event\"], test_labels_array[\"time\"], test_predictions)[0]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Concordance index (c-index) on test split: {c_index:.4f}\")\n",
    "\n",
    "    return final_model, cv_results, timing_info, c_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1.7 XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_opt_XGB(train_data, train_labels, test_data, test_labels,\n",
    "                  n_estimators_range=[50, 100, 200],\n",
    "                  max_depth_range=[3, 7, 10],\n",
    "                  cv_folds=5, verbose=True):\n",
    "    \"\"\"\n",
    "    XGBoost (XGB) Cox model hyperparameter optimization for survival analysis:\n",
    "        - Uses a fixed grid of hyperparameters for tuning.\n",
    "        - 5-fold CV along hyperparameter grid.\n",
    "        - Determines optimal settings for learning_rate, n_estimators, max_depth, etc.\n",
    "        - Re-trains model on the whole training split using optimal settings.\n",
    "        - Evaluates performance on the test split.\n",
    "        - Returns final refit model, CV results, timing information, and c-index for evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    warnings.simplefilter(\"ignore\", UserWarning)\n",
    "    warnings.simplefilter(\"ignore\", FitFailedWarning)\n",
    "\n",
    "    labels_array = np.array([(status, time) for status, time in zip(train_labels.iloc[:, 0], train_labels.iloc[:, 1])],\n",
    "                            dtype=[('event', '?'), ('time', '<f8')])\n",
    "\n",
    "    \n",
    "    # Step 1: Set up CV grid\n",
    "    param_grid = {\n",
    "        'n_estimators': n_estimators_range,\n",
    "        'max_depth': max_depth_range\n",
    "    }\n",
    "\n",
    "    cv = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Step 2: Hyperparameter optimisation\n",
    "    if verbose:\n",
    "        print(\"Starting hyperparameter optimization using GridSearchCV...\")\n",
    "\n",
    "    start_cv_time = time.time()\n",
    "    grid_search = GridSearchCV(\n",
    "        XGBRegressor(objective='survival:cox', eval_metric='cox-nloglik', tree_method='exact', \n",
    "                     learning_rate = 0.1, min_child_weight=10),\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=10 if verbose else 0,\n",
    "        pre_dispatch='2*n_jobs'\n",
    "    )\n",
    "\n",
    "    grid_search.fit(train_data, labels_array['time'])\n",
    "    end_cv_time = time.time()\n",
    "    cv_time = end_cv_time - start_cv_time\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Hyperparameter optimization took {cv_time:.2f} seconds.\")\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_n_estimators = grid_search.best_params_['n_estimators']\n",
    "    best_max_depth = grid_search.best_params_['max_depth']\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nBest n_estimators: {best_n_estimators}, \"\n",
    "              f\"Best max_depth: {best_max_depth}\")\n",
    "\n",
    "    # Step 3: Model fit\n",
    "    if verbose:\n",
    "        print(\"\\nRefitting the model on the entire training set with optimal parameters...\")\n",
    "\n",
    "    start_refit_time = time.time()\n",
    "    final_model = XGBRegressor(\n",
    "        objective='survival:cox',\n",
    "        eval_metric='cox-nloglik',\n",
    "        tree_method='exact',\n",
    "        learning_rate=0.1,\n",
    "        min_child_weight=10,\n",
    "        n_estimators=best_n_estimators,\n",
    "        max_depth=best_max_depth\n",
    "    )\n",
    "    final_model.fit(train_data, labels_array['time'])\n",
    "    end_refit_time = time.time()\n",
    "    refit_time = end_refit_time - start_refit_time\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Refitting took {refit_time:.2f} seconds.\")\n",
    "\n",
    "    cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "    timing_info = {\n",
    "        'cv_time': cv_time,\n",
    "        'refit_time': refit_time\n",
    "    }\n",
    "\n",
    "    # Step 4: Harrel's C on the test split\n",
    "    if verbose:\n",
    "        print(\"\\nEvaluating model performance on the test split...\")\n",
    "\n",
    "    test_labels_array = np.array([(status, time) for status, time in zip(test_labels.iloc[:, 0], test_labels.iloc[:, 1])],\n",
    "                                 dtype=[('event', '?'), ('time', '<f8')])\n",
    "\n",
    "    test_predictions = final_model.predict(test_data)\n",
    "\n",
    "    c_index = concordance_index_censored(test_labels_array[\"event\"], test_labels_array[\"time\"], test_predictions)[0]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Concordance index (c-index) on test split: {c_index:.4f}\")\n",
    "\n",
    "    return final_model, cv_results, timing_info, c_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1.8 DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DeepSurvWrapper(BaseEstimator):\n",
    "    def __init__(self, optimizer, input_dim, epochs=50, batch_size=32, lr=0.001, dropout=0.2, layer_sizes=[32,32], verbose=True):\n",
    "        \n",
    "        \"\"\"\n",
    "        Wrapper for integrating DeepSurv with sklearn's GridSearchCV.\n",
    "\n",
    "        Parameters:\n",
    "        - model: A PyTorch DeepSurv model instance.\n",
    "        - optimizer: A PyTorch optimizer class (e.g., torch.optim.Adam).\n",
    "        - epochs: Number of training epochs.\n",
    "        - batch_size: Batch size for training.\n",
    "        - lr: Learning rate for the optimizer.\n",
    "        - layer_sizes: Sizes of the layers (and thus also number of layers)\n",
    "        - verbose: Whether to print training progress.\n",
    "        \"\"\"\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.optimizer = optimizer\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "        self.dropout = dropout\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "        self.model = DeepSurvModel(self.input_dim, self.layer_sizes, self.dropout).to(self.device)  \n",
    "        torch.set_num_threads(24)\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            \"input_dim\": self.input_dim,\n",
    "            \"optimizer\": self.optimizer,\n",
    "            \"epochs\": self.epochs,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"lr\": self.lr,\n",
    "            \"verbose\": self.verbose,\n",
    "            \"dropout\": self.dropout,\n",
    "            \"layer_sizes\": self.layer_sizes\n",
    "        }\n",
    "\n",
    "    def fit(self, X, y, patience=5):\n",
    "        \n",
    "        \"\"\"\n",
    "        Train the DeepSurv model.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Features (numpy array or pandas DataFrame).\n",
    "        - y: Targets as a DataFrame with columns [\"CVD_status\", \"CVD_followup\"].\n",
    "          - \"CVD_status\": Binary array indicating whether the event occurred.\n",
    "          - \"CVD_followup\": Array of survival times (or censoring times).\n",
    "        - patience: Number of epochs to wait for improvement before stopping.\n",
    "        \"\"\"\n",
    "\n",
    "        #convert x and y to tensors\n",
    "        self.model.train()\n",
    "        X_tensor = torch.tensor(X.values, dtype=torch.float32).to(self.device)\n",
    "        event_tensor = torch.tensor(y.iloc[:,0].values, dtype=torch.float32).to(self.device)  # Event (0/1)\n",
    "        time_tensor = torch.tensor(y.iloc[:,1].values, dtype=torch.float32).to(self.device)  # Survival time\n",
    "\n",
    "        #optimizer and loss function\n",
    "        optimizer = self.optimizer(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "        best_train_loss = float('inf')\n",
    "        epochs_without_improvement = 0\n",
    "        \n",
    "        num_samples = X_tensor.size(0)\n",
    "        num_batches = int(np.ceil(num_samples / self.batch_size))\n",
    "\n",
    "\n",
    "        #training loop\n",
    "        for epoch in range(self.epochs):\n",
    "            epoch_loss = 0.0\n",
    "\n",
    "            #shuffle the data at the beginning of each epoch\n",
    "            permutation = torch.randperm(num_samples)\n",
    "            X_shuffled = X_tensor[permutation]\n",
    "            event_shuffled = event_tensor[permutation]\n",
    "            time_shuffled = time_tensor[permutation]\n",
    "\n",
    "            for batch_idx in range(num_batches):\n",
    "                start_idx = batch_idx * self.batch_size\n",
    "                end_idx = min(start_idx + self.batch_size, num_samples)\n",
    "\n",
    "                batch_X = X_shuffled[start_idx:end_idx]\n",
    "                batch_event = event_shuffled[start_idx:end_idx]\n",
    "                batch_time = time_shuffled[start_idx:end_idx]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                predictions = self.model(batch_X).squeeze()\n",
    "                loss = neg_partial_log_likelihood(predictions, batch_event.bool(), batch_time)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item() * batch_X.size(0)\n",
    "\n",
    "            #check for improvement on training loss\n",
    "            avg_epoch_loss = epoch_loss / num_samples\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"Epoch {epoch+1}/{self.epochs}, Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "            if avg_epoch_loss < best_train_loss:\n",
    "                best_train_loss = avg_epoch_loss\n",
    "                epochs_without_improvement = 0\n",
    "                #save the best model\n",
    "                best_model_state = self.model.state_dict()\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "    \n",
    "            #early stopping\n",
    "            if epochs_without_improvement >= patience:\n",
    "                if self.verbose:\n",
    "                    print(\"Early stopping due to no improvement in training loss\")\n",
    "                break\n",
    "    \n",
    "        #optionally load the weights of the best model\n",
    "        if 'best_model_state' in locals():\n",
    "            self.model.load_state_dict(best_model_state)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict risk scores for the given features.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Features (numpy array or pandas DataFrame).\n",
    "\n",
    "        Returns:\n",
    "        - Risk scores (numpy array).\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X.values, dtype=torch.float32).to(self.device)\n",
    "            risk_scores = self.model(X_tensor).squeeze().cpu().numpy()  # Squeeze to reduce unnecessary dimensions\n",
    "        return risk_scores\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute the concordance index as the evaluation metric.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Features (numpy array or pandas DataFrame).\n",
    "        - y: Targets as a DataFrame with columns [\"CVD_status\", \"CVD_followup\"].\n",
    "\n",
    "        Returns:\n",
    "        - Concordance index score (float).\n",
    "        \"\"\"\n",
    "        risk_scores = self.predict(X)\n",
    "        risk_scores_tensor = torch.tensor(risk_scores, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        #prepare event and time tensors\n",
    "        event_tensor = torch.tensor(y.iloc[:,0].values, dtype=torch.bool).to(self.device)  # Event (0/1)\n",
    "        time_tensor = torch.tensor(y.iloc[:,1].values, dtype=torch.float32).to(self.device)  # Survival time\n",
    "\n",
    "        cox_index = ConcordanceIndex()\n",
    "        c_index = cox_index(risk_scores_tensor, event_tensor, time_tensor)\n",
    "\n",
    "        return c_index\n",
    "\n",
    "\n",
    "#define model architecture\n",
    "class DeepSurvModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, layer_sizes, dropout):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim (int): Number of input features.\n",
    "            layer_sizes (list): List of integers where each integer specifies \n",
    "                                the number of neurons in a layer.\n",
    "            dropout (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super(DeepSurvModel, self).__init__()\n",
    "        \n",
    "        self.layers = torch.nn.ModuleList() \n",
    "        self.batch_norms = torch.nn.ModuleList()  # BatchNorm for each layer\n",
    "        \n",
    "        #add first batch normalization layer for input\n",
    "        self.batch_norms.append(torch.nn.BatchNorm1d(input_dim))\n",
    "        \n",
    "        #dynamically add linear, activation, and dropout layers\n",
    "        prev_dim = input_dim\n",
    "        for layer_size in layer_sizes:\n",
    "            self.layers.append(torch.nn.Linear(prev_dim, layer_size))\n",
    "            self.batch_norms.append(torch.nn.BatchNorm1d(layer_size))  #BatchNorm after each Linear layer\n",
    "            prev_dim = layer_size\n",
    "        \n",
    "        #output layer for final prediction\n",
    "        self.output = torch.nn.Linear(prev_dim, 1)  # Estimating log hazards\n",
    "        \n",
    "        #activation and dropout\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #apply batch norm to input\n",
    "        x = self.batch_norms[0](x)\n",
    "        \n",
    "        #pass through each linear layer with ReLU and Dropout\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            x = self.batch_norms[i + 1](x)  # BatchNorm after linear\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        #output layer\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_opt_DL(\n",
    "    train_data, train_labels, test_data, test_labels,\n",
    "    learning_rate_range=[0.001],\n",
    "    batch_size_range=[50000],\n",
    "    epoch_range=[100],\n",
    "    dropout_range=[0.2],\n",
    "    layer_numbers=[2, 3, 5],\n",
    "    layer_sizes=[16, 64, 256],\n",
    "    cv_folds=5, verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Optimized deep learning survival model hyperparameter tuning:\n",
    "        - Dynamically generates all combinations of uniform layer sizes for the specified number of layers.\n",
    "        - Uses GridSearchCV for hyperparameter optimization.\n",
    "        - Evaluates using concordance index (c-index).\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    \n",
    "    warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "    \n",
    "    ## Step 1: Set up CV grid\n",
    "    # Generate all possible uniform layer configurations\n",
    "    layer_configurations = [\n",
    "        [size] * num_layers\n",
    "        for num_layers in layer_numbers\n",
    "        for size in layer_sizes\n",
    "    ]\n",
    "\n",
    "    param_grid = {\n",
    "        'optimizer': [torch.optim.Adam],\n",
    "        'lr': learning_rate_range,\n",
    "        'batch_size': batch_size_range,\n",
    "        'epochs': epoch_range,\n",
    "        'dropouts': dropout_range,\n",
    "        'layer_sizes': layer_configurations\n",
    "    }\n",
    "\n",
    "    # Step 2: Hyperparameter optimisation\n",
    "    if verbose:\n",
    "        print(\"Starting hyperparameter optimization using GridSearchCV...\")\n",
    "    cv = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    start_cv_time = time.time()\n",
    "    deep_surv_wrapper = DeepSurvWrapper(\n",
    "        optimizer=torch.optim.Adam,\n",
    "        epochs=5,  # Placeholder, will be updated by GridSearchCV\n",
    "        batch_size=32,  # Placeholder\n",
    "        lr=1e-3,  # Placeholder,\n",
    "        dropout=0.1,  # Placeholder\n",
    "        verbose=verbose,\n",
    "        layer_sizes=[32, 32],  # Placeholder\n",
    "        input_dim=train_data.shape[1]\n",
    "    )\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=deep_surv_wrapper,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        n_jobs=1,\n",
    "        verbose=10 if verbose else 0\n",
    "    )\n",
    "    grid_search.fit(train_data, train_labels)\n",
    "    end_cv_time = time.time()\n",
    "    cv_time = end_cv_time - start_cv_time\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Hyperparameter optimization took {cv_time:.2f} seconds.\")\n",
    "\n",
    "    #get best model and parameters \n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nBest Parameters:\")\n",
    "        for param, value in best_params.items():\n",
    "            print(f\"{param}: {value}\")\n",
    "\n",
    "    # Step 3: Model fit\n",
    "    if verbose:\n",
    "        print(\"\\nRefitting the model on the entire training set with optimal parameters...\")\n",
    "\n",
    "    start_refit_time = time.time()\n",
    "    final_model = grid_search.best_estimator_\n",
    "    final_model.fit(train_data, train_labels)\n",
    "    end_refit_time = time.time()\n",
    "    refit_time = end_refit_time - start_refit_time\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Refitting took {refit_time:.2f} seconds.\")\n",
    "\n",
    "    cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "    timing_info = {\n",
    "        'cv_time': cv_time,\n",
    "        'refit_time': refit_time\n",
    "    }\n",
    "\n",
    "    # Step 4: Harrel's C on the test split\n",
    "    if verbose:\n",
    "        print(\"\\nEvaluating model performance on the test split...\")\n",
    "\n",
    "    #predict\n",
    "    test_predictions = final_model.predict(test_data)\n",
    "    test_predictions_tensor = torch.tensor(test_predictions, dtype=torch.float32)\n",
    "    event_tensor = torch.tensor(test_labels.iloc[:,0].values, dtype=torch.bool)  # Event (0/1)\n",
    "    time_tensor = torch.tensor(test_labels.iloc[:,1].values, dtype=torch.float32)  # Survival time\n",
    "    \n",
    "    #c-index\n",
    "    cox_index = ConcordanceIndex()\n",
    "    c_index = cox_index(test_predictions_tensor, event_tensor, time_tensor)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Concordance index (c-index) on test split: {c_index:.4f}\")\n",
    "\n",
    "    return final_model, cv_results, timing_info, c_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.2 Misc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.2.1 Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_and_prepare_data(file, endpoint, sample_size=None):\n",
    "    \"\"\"\n",
    "    Function to load the imputed dataset, filter by endpoint-specific rules, and prepare labels.\n",
    "    \n",
    "    Args:\n",
    "    - file (str): The file path to the imputed data split.\n",
    "    - endpoint (str): The endpoint to use for filtering data.\n",
    "    - sample_size (int, optional): Number of samples to retain. If None, use full dataset.\n",
    "\n",
    "    Returns:\n",
    "    - df_filtered (DataFrame): The filtered dataset.\n",
    "    - labels (DataFrame): The corresponding labels for modeling.\n",
    "    \"\"\"\n",
    "    # Download the imputed dataset\n",
    "    dl_cmd = f\"dx download '{file}' --overwrite\"\n",
    "    !{dl_cmd}\n",
    "\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file.split('/')[-1], sep=\"\\t\")\n",
    "\n",
    "    # Round continuous variables to 1 decimal place\n",
    "    continuous_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df[continuous_cols] = df[continuous_cols].round(2)\n",
    "\n",
    "    # Endpoint specific exclusion - baseline endpoint status\n",
    "    eids_to_include = df[df[f\"{endpoint}_at_base\"] == False][\"eid\"]\n",
    "    df_filtered = df[df[\"eid\"].isin(eids_to_include)]\n",
    "\n",
    "    # Sex-based exclusion\n",
    "    if endpoint == \"PC\":\n",
    "        eids_to_exclude = df[df[\"clinicalrisk_Sex_0\"] == True][\"eid\"]\n",
    "        df_filtered = df_filtered[~df_filtered[\"eid\"].isin(eids_to_exclude)]\n",
    "    elif endpoint == \"BC\":\n",
    "        eids_to_exclude = df[df[\"clinicalrisk_Sex_1\"] == True][\"eid\"]\n",
    "        df_filtered = df_filtered[~df_filtered[\"eid\"].isin(eids_to_exclude)]\n",
    "\n",
    "    if sample_size and len(df_filtered) > sample_size:\n",
    "        df_filtered = df_filtered.sample(n=sample_size, random_state=42)\n",
    "\n",
    "    labels = df_filtered[[f\"{endpoint}_status\", f\"{endpoint}_followup\", \"eid\", \"testtrain\"]].copy()\n",
    "    labels = labels.set_index(\"eid\")\n",
    "\n",
    "    return df_filtered, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def split_train_test(df_filtered, labels, testtrain_column='testtrain'):\n",
    "    \n",
    "    train_data = df_filtered[df_filtered[testtrain_column] == 'train'].drop(columns=[testtrain_column])\n",
    "    test_data = df_filtered[df_filtered[testtrain_column] == 'test'].drop(columns=[testtrain_column])\n",
    "\n",
    "    train_labels = labels[labels[testtrain_column] == 'train'].drop(columns=[testtrain_column])\n",
    "    test_labels = labels[labels[testtrain_column] == 'test'].drop(columns=[testtrain_column])\n",
    "\n",
    "    return train_data, test_data, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def remove_low_variance_features(train_data, test_data, threshold=0.01, verbose=True):\n",
    "    \"\"\"\n",
    "    Remove low variance and highly correlated features from train and test datasets.\n",
    "\n",
    "    Parameters:\n",
    "    - train_data (DataFrame): Training dataset.\n",
    "    - test_data (DataFrame): Test dataset.\n",
    "    - threshold (float): Threshold for variance below which features will be removed.\n",
    "    - verbose (bool): If True, prints out information about removed features.\n",
    "\n",
    "    Returns:\n",
    "    - train_data_reduced (DataFrame): Reduced training dataset.\n",
    "    - test_data_reduced (DataFrame): Reduced test dataset.\n",
    "    - retained_columns (Index): Names of the retained columns.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Separating boolean and numeric data...\")\n",
    "\n",
    "    # Separate boolean and numeric columns\n",
    "    bool_columns = train_data.select_dtypes(include=['bool']).columns\n",
    "    numeric_columns = train_data.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "    # Ensure no unsupported types are present\n",
    "    unsupported_columns = train_data.columns.difference(bool_columns.union(numeric_columns))\n",
    "    if not unsupported_columns.empty:\n",
    "        raise ValueError(f\"Unsupported data types found in columns: {list(unsupported_columns)}\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Boolean columns: {len(bool_columns)}\")\n",
    "        print(f\"Numeric columns: {len(numeric_columns)}\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Checking for missing or infinite values in numeric columns...\")\n",
    "\n",
    "    # Check for missing or infinite values in numeric columns only\n",
    "    assert not train_data[numeric_columns].isnull().any().any(), \"train_data contains NaN values in numeric columns\"\n",
    "    assert not np.isinf(train_data[numeric_columns].values).any(), \"train_data contains infinite values in numeric columns\"\n",
    "    assert not test_data[numeric_columns].isnull().any().any(), \"test_data contains NaN values in numeric columns\"\n",
    "    assert not np.isinf(test_data[numeric_columns].values).any(), \"test_data contains infinite values in numeric columns\"\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Removing low variance features...\")\n",
    "\n",
    "    # Remove low variance features (applies to all columns together)\n",
    "    selector = VarianceThreshold(threshold=threshold)\n",
    "    train_data_reduced = selector.fit_transform(train_data)\n",
    "    test_data_reduced = selector.transform(test_data)\n",
    "    retained_columns = train_data.columns[selector.get_support(indices=True)]\n",
    "\n",
    "    # Convert reduced numpy arrays back to DataFrame\n",
    "    train_data_reduced = pd.DataFrame(train_data_reduced, columns=retained_columns, index=train_data.index)\n",
    "    test_data_reduced = pd.DataFrame(test_data_reduced, columns=retained_columns, index=test_data.index)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Number of low variance features removed: {train_data.shape[1] - len(retained_columns)}\")\n",
    "        print(f\"Number of features retained: {len(retained_columns)}\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Removing highly correlated features from numeric columns...\")\n",
    "\n",
    "    # Remove highly correlated features (applies only to numeric columns)\n",
    "    correlation_matrix = train_data_reduced[numeric_columns].corr()\n",
    "    upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper_triangle.columns if any(abs(upper_triangle[column]) > 0.99)]\n",
    "\n",
    "    train_data_reduced = train_data_reduced.drop(columns=to_drop, errors='ignore')\n",
    "    test_data_reduced = test_data_reduced.drop(columns=to_drop, errors='ignore')\n",
    "    retained_columns = train_data_reduced.columns\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Number of highly correlated features removed: {len(to_drop)}\")\n",
    "        print(f\"Number of features retained after correlation filtering: {len(retained_columns)}\")\n",
    "\n",
    "    return train_data_reduced, test_data_reduced, retained_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AD', 'BC', 'CVD']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "endpoint_names = [\n",
    "    \"CVD\", \"HF\", \"BC\", \"DM\", \"LD\", \"RD\", \"AF\",  \"CAD\", \"VT\", \"ISS\", \"AAA\", \"PAD\", \n",
    "    \"AS\", \"COPD\", \"LC\", \"MEL\", \"CRC\", \"PC\", \"PD\", \"OP\", \"CAT\", \"POAG\", \"HT\", \"AD\" \n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "endpoint_names = [\n",
    "   \"AD\", \"BC\", \"CVD\" \n",
    "]\n",
    "\n",
    "print(endpoint_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.2.2 Saving & Uploading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def upload_model_cvresults_timing(model, model_type, endpoint, combo_name, cvresults, timing, cv_split, sample_size, directory=\"Benchmarking/ResultsScaling\"):\n",
    "    # Filenames for model, cross-validation results, and timing results\n",
    "    filename_model = f\"{model_type}_{endpoint}_{combo_name}_cvsplit_{cv_split}_n{sample_size}.pkl\"\n",
    "    filename_timings = f\"{model_type}_{endpoint}_{combo_name}_cvsplit_{cv_split}_n{sample_size}_timings.tsv\"\n",
    "    \n",
    "    # Save model\n",
    "    joblib.dump(model, filename_model)\n",
    "    upload_cmd_model = f\"dx upload {filename_model} --path {directory}/{filename_model}\"\n",
    "    !{upload_cmd_model}\n",
    "    \n",
    "    # Save cross-validation results if they exist\n",
    "    if cvresults is not None:\n",
    "        filename_cvresults = f\"{model_type}_{endpoint}_{combo_name}_cvsplit_{cv_split}_n{sample_size}_cvresults.tsv\"\n",
    "        cvresults.to_csv(filename_cvresults, sep='\\t', index=False)\n",
    "        upload_cmd_cvresults = f\"dx upload {filename_cvresults} --path {directory}/{filename_cvresults}\"\n",
    "        !{upload_cmd_cvresults}\n",
    "        os.remove(filename_cvresults)\n",
    "\n",
    "    # Save timing information\n",
    "    timing_df = pd.DataFrame([timing])\n",
    "    timing_df.to_csv(filename_timings, sep='\\t', index=False)\n",
    "    upload_cmd_timings = f\"dx upload {filename_timings} --path {directory}/{filename_timings}\"\n",
    "    !{upload_cmd_timings}\n",
    "\n",
    "    os.remove(filename_model)\n",
    "    os.remove(filename_timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_and_upload_lps_cindex(model, model_type, train_data, test_data, train_labels, test_labels, c_index, endpoint, combo_name, cv_split, sample_size, directory=\"Benchmarking/ResultsScaling\"):\n",
    "    \"\"\"\n",
    "    Save and upload linear predictors and concordance index for the Cox, LightGBM, and RangerRF models.\n",
    "    \"\"\"\n",
    "    if model_type == \"lightGBM\":\n",
    "        retained_columns = model.feature_name()\n",
    "    elif model_type in [\"DL\", \"RangerRF\"]:\n",
    "        retained_columns = train_data.columns\n",
    "    else:\n",
    "        retained_columns = model.feature_names_in_\n",
    "\n",
    "    train_data_reduced = train_data[retained_columns]\n",
    "    test_data_reduced = test_data[retained_columns]\n",
    "\n",
    "    train_lp = model.predict(train_data_reduced)\n",
    "    test_lp = model.predict(test_data_reduced)\n",
    "    \n",
    "    train_lp_df = pd.DataFrame({\"eid\": train_labels.index, \"LP\": train_lp})\n",
    "    test_lp_df = pd.DataFrame({\"eid\": test_labels.index, \"LP\": test_lp})\n",
    "\n",
    "    train_lp_filename = f\"{model_type}_{endpoint}_{combo_name}_cvsplit_{cv_split}_n{sample_size}_train_LP.tsv\"\n",
    "    test_lp_filename = f\"{model_type}_{endpoint}_{combo_name}_cvsplit_{cv_split}_n{sample_size}_test_LP.tsv\"\n",
    "    train_lp_df.to_csv(train_lp_filename, sep='\\t', index=False)\n",
    "    test_lp_df.to_csv(test_lp_filename, sep='\\t', index=False)\n",
    "    \n",
    "    performance_filename = f\"{model_type}_{endpoint}_{combo_name}_cvsplit_{cv_split}_n{sample_size}_performance.tsv\"\n",
    "    performance_df = pd.DataFrame([{'cv_split': cv_split, 'c_index': c_index}])\n",
    "    performance_df.to_csv(performance_filename, sep='\\t', index=False)\n",
    "\n",
    "    upload_cmd_trainlp = f\"dx upload {train_lp_filename} --path {directory}/{train_lp_filename}\"\n",
    "    upload_cmd_testlp = f\"dx upload {test_lp_filename} --path {directory}/{test_lp_filename}\"\n",
    "    upload_cmd_performance = f\"dx upload {performance_filename} --path {directory}/{performance_filename}\"\n",
    "\n",
    "    !{upload_cmd_trainlp}\n",
    "    !{upload_cmd_testlp}\n",
    "    !{upload_cmd_performance}\n",
    "\n",
    "    os.remove(train_lp_filename)\n",
    "    os.remove(test_lp_filename)\n",
    "    os.remove(performance_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_and_upload_feature_importance(model, model_type, train_data, train_labels, endpoint, combo_name, cv_split, sample_size, directory=\"Benchmarking/ResultsScaling\"):\n",
    "    \"\"\"\n",
    "    Save and upload feature importance or model coefficients for the trained model.\n",
    "    \"\"\"\n",
    "    retained_columns = train_data.columns\n",
    "\n",
    "    if model_type == \"RangerRF\":\n",
    "        train_labels_array = np.array([(status, time) for status, time in zip(train_labels.iloc[:, 0], train_labels.iloc[:, 1])],\n",
    "                                 dtype=[('event', '?'), ('time', '<f8')])\n",
    "        result = permutation_importance(model, \n",
    "                                        train_data, train_labels_array, \n",
    "                                        n_repeats=1, \n",
    "                                        random_state=42, \n",
    "                                        n_jobs=16)\n",
    "        coef_df = pd.DataFrame({\n",
    "            \"importance_mean\": result.importances_mean,\n",
    "            \"importance_std\": result.importances_std\n",
    "        }, index=retained_columns).sort_values(by=\"importance_mean\", ascending=False)\n",
    "    elif model_type in [\"EN\", \"Lasso\", \"Ridge\", \"Cox\"]:\n",
    "        coef_df = pd.DataFrame(model.coef_, index=retained_columns, columns=[\"Coefficient\"])\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type.\")\n",
    "        \n",
    "    coeff_filename = f\"{model_type}_{endpoint}_{combo_name}_cvsplit_{cv_split}_n{sample_size}_feature_importance.tsv\"\n",
    "    coef_df.to_csv(coeff_filename, sep='\\t')\n",
    "\n",
    "    upload_cmd_coef = f\"dx upload {coeff_filename} --path {directory}/{coeff_filename}\"\n",
    "    os.system(upload_cmd_coef)\n",
    "\n",
    "    os.remove(coeff_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.2.3 Predictor combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "always_include = [\"clinicalrisk_Age.at.recruitment\", \"clinicalrisk_Sex_1\", \"eid\", \"testtrain\"]\n",
    "\n",
    "predictor_combinations = {\n",
    "    \"prs_metabolomics\": [\"prs_\", \"metabolomics_\"],\n",
    "    \"pmh\": [\"pmh_\"],\n",
    "    #\"ts\": [\"ts_\"],\n",
    "    #\"metabolomics\": [\"metabolomics_\"],\n",
    "    #\"prs\": [\"prs_\"],\n",
    "    \"clinicalrisk\": [\"clinicalrisk_\"],\n",
    "    #\"pmh_ts\": [\"pmh_\", \"ts_\"],\n",
    "    #\"prs_metabolomics_pmh_ts\": [\"prs_\", \"metabolomics_\", \"pmh_\", \"ts_\"],\n",
    "    #\"clinicalrisk_pmh_ts\": [\"clinicalrisk_\", \"pmh_\", \"ts_\"],\n",
    "    #\"clinicalrisk_prs_metabolomics\": [\"clinicalrisk_\", \"prs_\", \"metabolomics_\"],\n",
    "    \"everything\": [\"clinicalrisk_\", \"pmh_\", \"prs_\", \"metabolomics_\"],\n",
    "    #\"score\": [\"score_\"],\n",
    "    #\"qrisk\": [\"qrisk_\"],\n",
    "    #\"prevent\": [\"prevent_\"],\n",
    "    \"agesex\": []\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Final Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.1 for everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEverything was conducted on mem3_ssd1_v2_x32 instances\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Everything was conducted on mem3_ssd1_v2_x32 instances\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting CV Split: 1\n",
      "Processing Endpoint: AD\n",
      "\u001b[2K[===========================================================>] Completed 1,626,953,279 of 1,626,953,279 bytes (100%) /opt/notebooks/Processed_final_split_1_16012024.tsvv\n",
      "Processed 5000 rows for testing.\n",
      "Analyzing Combination: prs_metabolomics\n",
      "Analyzing Combination: pmh\n",
      "Analyzing Combination: clinicalrisk\n",
      "Analyzing Combination: everything\n",
      "Separating boolean and numeric data...\n",
      "Boolean columns: 447\n",
      "Numeric columns: 236\n",
      "Checking for missing or infinite values in numeric columns...\n",
      "Removing low variance features...\n",
      "Number of low variance features removed: 364\n",
      "Number of features retained: 319\n",
      "Removing highly correlated features from numeric columns...\n",
      "Number of highly correlated features removed: 42\n",
      "Number of features retained after correlation filtering: 277\n",
      "Training model: RangerRF\n",
      "\n",
      "Testing num_trees=50, max_depth=3...\n",
      "\n",
      "Testing num_trees=50, max_depth=7...\n",
      "\n",
      "Testing num_trees=50, max_depth=10...\n",
      "\n",
      "Testing num_trees=100, max_depth=3...\n",
      "\n",
      "Testing num_trees=100, max_depth=7...\n",
      "\n",
      "Testing num_trees=100, max_depth=10...\n",
      "\n",
      "Testing num_trees=200, max_depth=3...\n",
      "\n",
      "Testing num_trees=200, max_depth=7...\n",
      "\n",
      "Testing num_trees=200, max_depth=10...\n",
      "\n",
      "Best num_trees: 200, Best max_depth: 3\n",
      "Refitting took 3.48 seconds.\n",
      "Concordance index (c-index) on test split: 0.7285\n",
      "[===========================================================>] Uploaded 12,514,365 of 12,514,365 bytes (100%) RangerRF_AD_everything_cvsplit_1_n5000.pkl\n",
      "ID                                file-GyPVYFjJY757Q1q0Y149yYPg\n",
      "Class                             file\n",
      "Project                           project-GQ7Z9YjJY75053Jby60GXKFX\n",
      "Folder                            /Benchmarking/ResultsScaling\n",
      "Name                              RangerRF_AD_everything_cvsplit_1_n5000.pkl\n",
      "State                             \u001b[33mclosing\u001b[0m\n",
      "Visibility                        visible\n",
      "Types                             -\n",
      "Properties                        -\n",
      "Tags                              -\n",
      "Outgoing links                    -\n",
      "Created                           Sun Feb  2 07:07:27 2025\n",
      "Created by                        rafaelo\n",
      " via the job                      job-GyPVJPQJY753Q5jzv283VQ6G\n",
      "Last modified                     Sun Feb  2 07:07:28 2025\n",
      "Media type                        \n",
      "archivalState                     \"live\"\n",
      "cloudAccount                      \"cloudaccount-dnanexus\"\n",
      "[===========================================================>] Uploaded 428 of 428 bytes (100%) RangerRF_AD_everything_cvsplit_1_n5000_cvresults.tsv\n",
      "ID                                file-GyPVYG8JY7584qP1825Z9qjb\n",
      "Class                             file\n",
      "Project                           project-GQ7Z9YjJY75053Jby60GXKFX\n",
      "Folder                            /Benchmarking/ResultsScaling\n",
      "Name                              RangerRF_AD_everything_cvsplit_1_n5000_cvresults.tsv\n",
      "State                             \u001b[33mclosing\u001b[0m\n",
      "Visibility                        visible\n",
      "Types                             -\n",
      "Properties                        -\n",
      "Tags                              -\n",
      "Outgoing links                    -\n",
      "Created                           Sun Feb  2 07:07:29 2025\n",
      "Created by                        rafaelo\n",
      " via the job                      job-GyPVJPQJY753Q5jzv283VQ6G\n",
      "Last modified                     Sun Feb  2 07:07:30 2025\n",
      "Media type                        \n",
      "archivalState                     \"live\"\n",
      "cloudAccount                      \"cloudaccount-dnanexus\"\n",
      "[===========================================================>] Uploaded 29 of 29 bytes (100%) RangerRF_AD_everything_cvsplit_1_n5000_timings.tsv\n",
      "ID                                file-GyPVYGjJY753jp49pJ7f4QX5\n",
      "Class                             file\n",
      "Project                           project-GQ7Z9YjJY75053Jby60GXKFX\n",
      "Folder                            /Benchmarking/ResultsScaling\n",
      "Name                              RangerRF_AD_everything_cvsplit_1_n5000_timings.tsv\n",
      "State                             \u001b[33mclosing\u001b[0m\n",
      "Visibility                        visible\n",
      "Types                             -\n",
      "Properties                        -\n",
      "Tags                              -\n",
      "Outgoing links                    -\n",
      "Created                           Sun Feb  2 07:07:31 2025\n",
      "Created by                        rafaelo\n",
      " via the job                      job-GyPVJPQJY753Q5jzv283VQ6G\n",
      "Last modified                     Sun Feb  2 07:07:32 2025\n",
      "Media type                        \n",
      "archivalState                     \"live\"\n",
      "cloudAccount                      \"cloudaccount-dnanexus\"\n",
      "[===========================================================>] Uploaded 106,182 of 106,182 bytes (100%) RangerRF_AD_everything_cvsplit_1_n5000_train_LP.tsv\n",
      "ID                                file-GyPVYJQJY7584qP1825Z9qjg\n",
      "Class                             file\n",
      "Project                           project-GQ7Z9YjJY75053Jby60GXKFX\n",
      "Folder                            /Benchmarking/ResultsScaling\n",
      "Name                              RangerRF_AD_everything_cvsplit_1_n5000_train_LP.tsv\n",
      "State                             \u001b[33mclosing\u001b[0m\n",
      "Visibility                        visible\n",
      "Types                             -\n",
      "Properties                        -\n",
      "Tags                              -\n",
      "Outgoing links                    -\n",
      "Created                           Sun Feb  2 07:07:34 2025\n",
      "Created by                        rafaelo\n",
      " via the job                      job-GyPVJPQJY753Q5jzv283VQ6G\n",
      "Last modified                     Sun Feb  2 07:07:35 2025\n",
      "Media type                        \n",
      "archivalState                     \"live\"\n",
      "cloudAccount                      \"cloudaccount-dnanexus\"\n",
      "[===========================================================>] Uploaded 25,783 of 25,783 bytes (100%) RangerRF_AD_everything_cvsplit_1_n5000_test_LP.tsv\n",
      "ID                                file-GyPVYK0JY752VQbxg2pVyqky\n",
      "Class                             file\n",
      "Project                           project-GQ7Z9YjJY75053Jby60GXKFX\n",
      "Folder                            /Benchmarking/ResultsScaling\n",
      "Name                              RangerRF_AD_everything_cvsplit_1_n5000_test_LP.tsv\n",
      "State                             \u001b[33mclosing\u001b[0m\n",
      "Visibility                        visible\n",
      "Types                             -\n",
      "Properties                        -\n",
      "Tags                              -\n",
      "Outgoing links                    -\n",
      "Created                           Sun Feb  2 07:07:36 2025\n",
      "Created by                        rafaelo\n",
      " via the job                      job-GyPVJPQJY753Q5jzv283VQ6G\n",
      "Last modified                     Sun Feb  2 07:07:36 2025\n",
      "Media type                        \n",
      "archivalState                     \"live\"\n",
      "cloudAccount                      \"cloudaccount-dnanexus\"\n",
      "[===========================================================>] Uploaded 37 of 37 bytes (100%) RangerRF_AD_everything_cvsplit_1_n5000_performance.tsv\n",
      "ID                                file-GyPVYK8JY7587QggYx9B3630\n",
      "Class                             file\n",
      "Project                           project-GQ7Z9YjJY75053Jby60GXKFX\n",
      "Folder                            /Benchmarking/ResultsScaling\n",
      "Name                              RangerRF_AD_everything_cvsplit_1_n5000_performance.tsv\n",
      "State                             \u001b[33mclosing\u001b[0m\n",
      "Visibility                        visible\n",
      "Types                             -\n",
      "Properties                        -\n",
      "Tags                              -\n",
      "Outgoing links                    -\n",
      "Created                           Sun Feb  2 07:07:37 2025\n",
      "Created by                        rafaelo\n",
      " via the job                      job-GyPVJPQJY753Q5jzv283VQ6G\n",
      "Last modified                     Sun Feb  2 07:07:38 2025\n",
      "Media type                        \n",
      "archivalState                     \"live\"\n",
      "cloudAccount                      \"cloudaccount-dnanexus\"\n",
      "Analyzing Combination: agesex\n",
      "\u001b[2K[===========================================================>] Completed 1,626,953,279 of 1,626,953,279 bytes (100%) /opt/notebooks/Processed_final_split_1_16012024.tsvsv\n",
      "Processed 10000 rows for testing.\n",
      "Analyzing Combination: prs_metabolomics\n",
      "Analyzing Combination: pmh\n",
      "Analyzing Combination: clinicalrisk\n",
      "Analyzing Combination: everything\n",
      "Separating boolean and numeric data...\n",
      "Boolean columns: 447\n",
      "Numeric columns: 236\n",
      "Checking for missing or infinite values in numeric columns...\n",
      "Removing low variance features...\n",
      "Number of low variance features removed: 369\n",
      "Number of features retained: 314\n",
      "Removing highly correlated features from numeric columns...\n",
      "Number of highly correlated features removed: 42\n",
      "Number of features retained after correlation filtering: 272\n",
      "Training model: RangerRF\n",
      "\n",
      "Testing num_trees=50, max_depth=3...\n",
      "\n",
      "Testing num_trees=50, max_depth=7...\n",
      "\n",
      "Testing num_trees=50, max_depth=10...\n",
      "\n",
      "Testing num_trees=100, max_depth=3...\n",
      "\n",
      "Testing num_trees=100, max_depth=7...\n",
      "\n",
      "Testing num_trees=100, max_depth=10...\n"
     ]
    }
   ],
   "source": [
    "cv_splits = [1,2,3,4,5]  # Specify the actual CV splits to process\n",
    "processed_files = [f\"Benchmarking/Processed/Processed_final_split_{i}_16012024.tsv\" for i in cv_splits]\n",
    "sample_sizes = [5000, 10000, 20000, 50000]\n",
    "model_types = [\"Cox\", \"EN\", \"Lasso\", \"Ridge\", \"GBM\", \"lightGBM\", \"DL\", \"RangerRF\", \"XGB\"]\n",
    "model_types = [\"RangerRF\"]\n",
    "\n",
    "\n",
    "for cv_split, file in zip(cv_splits, processed_files):\n",
    "    print(f\"Starting CV Split: {cv_split}\")\n",
    "\n",
    "    for endpoint in endpoint_names:\n",
    "        print(f\"Processing Endpoint: {endpoint}\")\n",
    "\n",
    "        for sample_size in sample_sizes:\n",
    "            df_filtered, labels = load_and_prepare_data(file, endpoint, sample_size)\n",
    "            print(f\"Processed {len(df_filtered)} rows for testing.\")\n",
    "\n",
    "            for combo_name, prefixes in predictor_combinations.items():\n",
    "                print(f\"Analyzing Combination: {combo_name}\")\n",
    "                if combo_name not in [\"everything\"]: continue\n",
    "                \n",
    "                selected_cols = always_include + [\n",
    "                    col for col in df_filtered.columns\n",
    "                    if any(col.startswith(prefix) for prefix in prefixes) and col not in always_include\n",
    "                ]\n",
    "                df_filtered2 = df_filtered[selected_cols]\n",
    "                df_filtered2 = df_filtered2.set_index(\"eid\").replace({'TRUE': 1, 'FALSE': 0})\n",
    "\n",
    "                train_data, test_data, train_labels, test_labels = split_train_test(df_filtered2, labels)\n",
    "                train_data, test_data, retained_columns = remove_low_variance_features(train_data, test_data, threshold=0.01)\n",
    "                \n",
    "                for model_type in model_types:\n",
    "                    print(f\"Training model: {model_type}\")\n",
    "\n",
    "                    if model_type == \"EN\":\n",
    "                        best_model, results_df, timing, cindex = train_opt_EN(train_data, train_labels, test_data, test_labels)\n",
    "                    elif model_type == \"Lasso\":\n",
    "                        best_model, results_df, timing, cindex = train_opt_lasso(train_data, train_labels, test_data, test_labels)\n",
    "                    elif model_type == \"Ridge\":\n",
    "                        best_model, results_df, timing, cindex = train_opt_ridge(train_data, train_labels, test_data, test_labels)\n",
    "                    elif model_type == \"Cox\":\n",
    "                        best_model, timing, cindex = train_cox(train_data, train_labels, test_data, test_labels)\n",
    "                        results_df = None\n",
    "                    elif model_type == \"RF\":\n",
    "                        best_model, results_df, timing, cindex = train_opt_RF(train_data, train_labels, test_data, test_labels)\n",
    "                    elif model_type == \"GBM\":\n",
    "                        best_model, results_df, timing, cindex = train_opt_GBM(train_data, train_labels, test_data, test_labels)\n",
    "                    elif model_type == \"lightGBM\":\n",
    "                        best_model, results_df, timing, cindex = train_opt_lightGBM(train_data, train_labels, test_data, test_labels)\n",
    "                    elif model_type == \"DL\":\n",
    "                        best_model, results_df, timing, cindex = train_opt_DL(train_data, train_labels, test_data, test_labels)\n",
    "                    elif model_type == \"RangerRF\":\n",
    "                        best_model, results_df, timing, cindex = train_opt_RangerRF(train_data, train_labels, test_data, test_labels)\n",
    "                    elif model_type == \"XGB\":\n",
    "                        best_model, results_df, timing, cindex = train_opt_XGB(train_data, train_labels, test_data, test_labels)\n",
    "                    else:\n",
    "                        raise ValueError(\"Unsupported model type.\")\n",
    "\n",
    "                    upload_model_cvresults_timing(best_model, model_type, endpoint, combo_name, results_df, timing, cv_split, sample_size)\n",
    "                    save_and_upload_lps_cindex(best_model, model_type, train_data, test_data, train_labels, test_labels, cindex, endpoint, combo_name, cv_split, sample_size)\n",
    "\n",
    "print(\"All tasks completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
